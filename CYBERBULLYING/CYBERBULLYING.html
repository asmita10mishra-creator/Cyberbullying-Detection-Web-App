<!DOCTYPE html>
<html lang="en" class="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cyberbullying Detection</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        slate: {
                            50: '#f8fafc',
                            100: '#f1f5f9',
                            200: '#e2e8f0',
                            300: '#cbd5e1',
                            400: '#94a3b8',
                            500: '#64748b',
                            600: '#475569',
                            700: '#334155',
                            800: '#1e293b',
                            900: '#0f172a',
                            950: '#020617',
                        }
                    }
                }
            }
        }
    </script>
    <style>
        body {
            transition: background-color 0.2s ease;
        }
        
        /* Bullying Detector component styles */
        .detector-container {
            background-color: white;
            border-radius: 0.5rem;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            padding: 1.5rem;
        }
        
        .dark .detector-container {
            background-color: rgb(15, 23, 42);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }
        
        .detector-textarea {
            width: 100%;
            min-height: 120px;
            padding: 0.75rem;
            border-radius: 0.375rem;
            border: 1px solid #e2e8f0;
            background-color: white;
            color: #1e293b;
        }
        
        .dark .detector-textarea {
            background-color: rgb(2, 6, 23);
            border-color: rgba(255, 255, 255, 0.1);
            color: #e2e8f0;
        }
        
        .detector-button {
            background-color: #3b82f6;
            color: white;
            padding: 0.5rem 1rem;
            border-radius: 0.375rem;
            font-weight: 500;
            margin-top: 1rem;
            cursor: pointer;
            transition: background-color 0.2s;
        }
        
        .detector-button:hover {
            background-color: #2563eb;
        }
        
        .result-container {
            margin-top: 1rem;
            padding: 1rem;
            border-radius: 0.375rem;
        }
        
        .result-bullying {
            background-color: #fee2e2;
            color: #b91c1c;
        }
        
        .dark .result-bullying {
            background-color: rgba(220, 38, 38, 0.1);
            color: #fca5a5;
        }
        
        .result-not-bullying {
            background-color: #dcfce7;
            color: #166534;
        }
        
        .dark .result-not-bullying {
            background-color: rgba(34, 197, 94, 0.1);
            color: #86efac;
        }
        
        .result-neutral {
            background-color: #e0f2fe;
            color: #0369a1;
        }
        
        .dark .result-neutral {
            background-color: rgba(14, 165, 233, 0.1);
            color: #7dd3fc;
        }
        
        .toggle-container {
            position: fixed;
            top: 1rem;
            right: 1rem;
        }
        
        .toggle-button {
            background-color: #e2e8f0;
            border-radius: 9999px;
            width: 3rem;
            height: 1.5rem;
            position: relative;
            cursor: pointer;
        }
        
        .dark .toggle-button {
            background-color: #334155;
        }
        
        .toggle-circle {
            background-color: white;
            border-radius: 9999px;
            width: 1.25rem;
            height: 1.25rem;
            position: absolute;
            top: 0.125rem;
            left: 0.125rem;
            transition: transform 0.2s;
        }
        
        .dark .toggle-circle {
            transform: translateX(1.5rem);
        }
    </style>
</head>
<body class="min-h-screen bg-gradient-to-b from-slate-50 to-slate-100 dark:from-slate-950 dark:to-slate-900">
    <div class="toggle-container">
        <div id="themeToggle" class="toggle-button">
            <div class="toggle-circle"></div>
        </div>
    </div>
    
    <div class="container mx-auto px-4 py-16">
        <div class="max-w-3xl mx-auto">
            <div class="text-center mb-12">
                <h1 class="text-4xl font-bold tracking-tight text-slate-900 dark:text-slate-50 sm:text-5xl mb-4">
                    Cyberbullying Detection
                </h1>
                <p class="text-lg text-slate-600 dark:text-slate-300">
                    Enter a sentence to analyze if it contains potential cyberbullying content.
                </p>
            </div>

            <div id="bullyingDetector" class="detector-container">
                <textarea id="detectorTextarea" class="detector-textarea" placeholder="Enter text to analyze..."></textarea>
                <button id="analyzeButton" class="detector-button">Analyze Text</button>
                <div id="resultContainer" class="result-container hidden"></div>
            </div>

            <div class="mt-16 space-y-6 text-slate-700 dark:text-slate-300">
                <div>
                    <h2 class="text-2xl font-semibold mb-3 text-slate-900 dark:text-slate-50">About This Tool</h2>
                    <p>
                        This tool uses AI to analyze text and identify potential cyberbullying content. It can help moderate
                        online conversations and create safer digital spaces.
                    </p>
                </div>

                <div>
                    <h2 class="text-2xl font-semibold mb-3 text-slate-900 dark:text-slate-50">How It Works</h2>
                    <p>
                        Our system analyzes the linguistic patterns, sentiment, and context of the text to determine if it
                        contains harmful content typically associated with cyberbullying.
                    </p>
                </div>

                <div>
                    <h2 class="text-2xl font-semibold mb-3 text-slate-900 dark:text-slate-50">Responsible Use</h2>
                    <p>
                        This tool is meant for educational purposes and to help create safer online environments. Please use it
                        responsibly and remember that AI predictions are not perfect.
                    </p>
                </div>
            </div>
        </div>
    </div>

    <footer class="border-t border-slate-200 dark:border-slate-800 py-8 mt-16">
        <div class="container mx-auto px-4 text-center text-slate-500 dark:text-slate-400">
            <p id="footerYear">Cyberbullying Detection Tool &copy; </p>
        </div>
    </footer>

<script>
    // Set current year in footer
    document.getElementById('footerYear').textContent += new Date().getFullYear();

    // Theme toggle functionality
    const themeToggle = document.getElementById('themeToggle');
    themeToggle.addEventListener('click', () => {
        document.documentElement.classList.toggle('dark');
        localStorage.setItem('darkMode', document.documentElement.classList.contains('dark'));
    });

    // Check for saved theme preference
    if (localStorage.getItem('darkMode') === 'true') {
        document.documentElement.classList.add('dark');
    } else if (localStorage.getItem('darkMode') === 'false') {
        document.documentElement.classList.remove('dark');
    }

    // Bullying detector functionality (connected to backend)
    const analyzeButton = document.getElementById('analyzeButton');
    const detectorTextarea = document.getElementById('detectorTextarea');
    const resultContainer = document.getElementById('resultContainer');

    analyzeButton.addEventListener('click', async () => {
        const text = detectorTextarea.value.trim();

        if (!text) {
            showResult('Please enter some text to analyze.', 'neutral');
            return;
        }

        try {
            const response = await fetch("http://127.0.0.1:5000/analyze", {
                method: "POST",
                headers: {
                    "Content-Type": "application/json"
                },
                body: JSON.stringify({ text })
            });

            if (!response.ok) {
                throw new Error("Network response was not ok");
            }

            const data = await response.json();
            const prediction = data.prediction;

            if (prediction === 1 || prediction === "cyberbullying") {
                showResult('This text appears to contain cyberbullying content. Please consider revising your message.', 'bullying');
            } else {
                showResult('This text does not appear to contain cyberbullying content.', 'not-bullying');
            }

        } catch (error) {
            console.error("Error during fetch:", error);
            showResult('An error occurred while analyzing the text.', 'neutral');
        }
    });

    function showResult(message, type) {
        resultContainer.textContent = message;
        resultContainer.className = 'result-container';
        resultContainer.classList.add(`result-${type}`);
        resultContainer.classList.remove('hidden');
    }
</script>

</body>
</html>